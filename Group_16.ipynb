{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Name: Group 16\n",
    "\n",
    "Group Members: Adam Jansen, Jacob King, Nikhil Pyndiah\n",
    "\n",
    "Group Emails: A: ajans048@uottawa.ca J: jking033@uottaw.ca N: npynd031@uottwa.ca\n",
    "\n",
    "Group Student #: A:300076841 J:300082223 N:300090426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FN score he got was around 90% in macro, more then 80\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# we can use the LabelEncoder to encode the gender feature\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV, cross_validate\n",
    "\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier #Will identify the maority calss base line, model needs to do better then the baseline\n",
    "\n",
    "# oversample the minority class using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# to reduce randomness then you put the seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data loading and exploratory analysis (18/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data shape: \n",
      "(615, 14)\n",
      "\n",
      "Test Data size: \n",
      "8610\n",
      "\n",
      "Test Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "dataframe columns: \n",
      "['Age' 'Sex' 'ALB' 'ALP' 'ALT' 'AST' 'BIL' 'CHE' 'CHOL' 'CREA' 'GGT'\n",
      " 'PROT']\n",
      "\n",
      "dataframe datatypes:\n",
      "\n",
      "Age           int64\n",
      "Sex          object\n",
      "ALB         float64\n",
      "ALP         float64\n",
      "ALT         float64\n",
      "AST         float64\n",
      "BIL         float64\n",
      "CHE         float64\n",
      "CHOL        float64\n",
      "CREA        float64\n",
      "GGT         float64\n",
      "PROT        float64\n",
      "split        object\n",
      "category      int64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Sex Distribution:\n",
      "m    377\n",
      "f    238\n",
      "Name: Sex, dtype: int64\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "dataframe class distribution:\n",
      "\n",
      "0 = HepC Negative, 1 = HepC Positive:\n",
      "\n",
      "0    540\n",
      "1     75\n",
      "Name: category, dtype: int64\n",
      "Percent With HepC: \n",
      "0.12195121951219512\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Median Age without Hep C:\n",
      "category\n",
      "0    47.0\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Age with Hep C:\n",
      "category\n",
      "1    48.426667\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Test Data shape: \n",
      "(185, 14)\n",
      "\n",
      "Test Data size: \n",
      "2590\n",
      "\n",
      "Test Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Train Data shape: \n",
      "(430, 14)\n",
      "\n",
      "Train Data size: \n",
      "6020\n",
      "\n",
      "Train Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Train DF class distribution:\n",
      "\n",
      "Percent With HepC: \n",
      "0.12325581395348838\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Test DF class distribution:\n",
      "\n",
      "Percent With HepC: \n",
      "0.11891891891891893\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "[('ALB', 'PROT', 0.557196911828285), ('AST', 'category', 0.6217239773457609)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the dataset (1)\n",
    "df = pd.read_csv('./data/hcv_data_split.csv')\n",
    "# print the dimensionality of the dataframe (1)\n",
    "print(f\"Test Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Test Data size: \\n{df.size}\\n\")\n",
    "print(f\"Test Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the names of the columns that can be used as features when training the machine learning model (1)\n",
    "columnArray = df.copy().drop(['category','split'],axis=1)\n",
    "columnArray = columnArray.columns.values\n",
    "print(f\"dataframe columns: \\n{columnArray}\\n\")\n",
    "\n",
    "# print the different data types that can be identified from the entire dataset (1)\n",
    "print(f\"dataframe datatypes:\\n\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the gender distribution in the complete dataset(i.e., the number of male and female individuals) (1)\n",
    "print(f\"Sex Distribution:\\n{df['Sex'].value_counts()}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the class distribution of the entire dataset (1)\n",
    "print(f\"dataframe class distribution:\\n\")\n",
    "print(f\"0 = HepC Negative, 1 = HepC Positive:\\n\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"Percent With HepC: \\n{df['category'].value_counts()[1]/ df.shape[0] }\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the median age of patients in the dataset having the hepatitis C infection (1.5)\n",
    "print(f\"Median Age without Hep C:\\n{df.groupby(['category']).median().head(1)['Age']}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the mean age of individuals in the dataset who does not have hepatitis C infection(i.e., the control group) (1.5)\n",
    "print(f\"Mean Age with Hep C:\\n{df.groupby(['category']).mean().tail(1)['Age']}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# split the dataset into train and test based on the field \"split\" (0.5 + 0.5)\n",
    "df.sort_values (by=[\"split\"])\n",
    "splitCounts = (df['split'].value_counts())\n",
    "testNum = (splitCounts.iloc[1])\n",
    "trainNum = (splitCounts.iloc[0])\n",
    "\n",
    "train_df = df.copy().head(trainNum)\n",
    "test_df = df.copy().tail(testNum)\n",
    "\n",
    "# print the dimensionality of the test dataset (0.5)\n",
    "print(f\"Test Data shape: \\n{test_df.shape}\\n\")\n",
    "print(f\"Test Data size: \\n{test_df.size}\\n\")\n",
    "print(f\"Test Data ndim: \\n{test_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the dimensionality of the training dataset (0.5)\n",
    "\n",
    "print(f\"Train Data shape: \\n{train_df.shape}\\n\")\n",
    "print(f\"Train Data size: \\n{train_df.size}\\n\")\n",
    "print(f\"Train Data ndim: \\n{train_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the proportional distribution of the classes to identify whether or not the classes are equally(or closer) distributed between the train and test datasets (1 + 1)\n",
    "print(f\"Train DF class distribution:\\n\")\n",
    "print(f\"Percent With HepC: \\n{train_df['category'].value_counts()[1]/ train_df.shape[0] }\\n\") #The Train dataset has a slight (~0.4%) over-representation of Hep-C positive rows\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "print(f\"Test DF class distribution:\\n\")\n",
    "print(f\"Percent With HepC: \\n{test_df['category'].value_counts()[1]/ test_df.shape[0] }\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# analyze the distribution of the individual features(i.e., by using the complete dataset) and plot a feature that has a rough approximation of a Gaussian distribution (2)\n",
    "df.hist() # Complete dataset plot\n",
    "\n",
    "df[\"CHE\"].hist() #Most gaussian feature\n",
    "\n",
    "# identify features that represent a notable correlation (i.e., either positive or negative correlation below or above -0.5 and 0.5) (3)\n",
    "corr_features = []\n",
    "corr_matrix = df.corr()\n",
    "for col in corr_matrix.columns: #Iterates over the columns in the correlation matrix\n",
    "    for ind in corr_matrix.index: #iterates over the rows in the correlation matrix\n",
    "        if((corr_matrix.at[ind,col]>0.5 or corr_matrix.at[ind,col]<-0.5) and  corr_matrix.at[ind,col]!=1.0 ): #Finds significant correlations within the above matrix\n",
    "            \n",
    "            item = (col,ind,corr_matrix.at[ind,col])\n",
    "            if (item not in corr_features and (item[1],item[0],item[2]) not in corr_features): # If a certain strong correlation (and its reverse) are not recorded, add it to the array\n",
    "                corr_features+= [item]\n",
    "print (corr_features)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model development (64/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      "     category\n",
      "0           0\n",
      "1           1\n",
      "2           0\n",
      "3           0\n",
      "4           1\n",
      "..        ...\n",
      "610         0\n",
      "611         0\n",
      "612         0\n",
      "613         0\n",
      "614         0\n",
      "\n",
      "[615 rows x 1 columns]\n",
      "\n",
      "Features: \n",
      "     Age Sex   ALB    ALP    ALT   AST    BIL    CHE  CHOL   CREA    GGT  \\\n",
      "0     59   m  37.8   83.7   25.3  20.0   18.6   7.52  5.07  108.0   17.4   \n",
      "1     41   m  31.0   85.3    4.8  60.2  200.0   1.80  5.34  106.4  151.0   \n",
      "2     76   m  29.2   48.9   25.2  27.2    8.3   4.52  2.79  127.0   18.3   \n",
      "3     51   f  47.4  117.3   62.1  30.4    3.8  10.43  6.59   86.0   69.3   \n",
      "4     59   f  36.0    NaN  100.0  80.0   12.0   9.07  5.30   67.0   34.0   \n",
      "..   ...  ..   ...    ...    ...   ...    ...    ...   ...    ...    ...   \n",
      "610   47   m  48.0   66.5   17.5  23.2    9.9   7.09  5.06   81.0   14.9   \n",
      "611   47   f  40.3   65.0   13.5  15.2    6.4   7.16  4.55   70.0   16.5   \n",
      "612   42   f  43.4   54.0   11.3  21.3    1.8   6.43  4.43   54.0   18.6   \n",
      "613   46   f  39.9   73.9   14.0  17.2   16.3   6.93  5.11   71.0   12.7   \n",
      "614   49   f  39.1   89.4   15.4  24.1    4.1  10.03  8.36   74.0   12.0   \n",
      "\n",
      "     PROT  split  \n",
      "0    64.1  train  \n",
      "1    71.8  train  \n",
      "2    58.1  train  \n",
      "3    71.0  train  \n",
      "4    68.0  train  \n",
      "..    ...    ...  \n",
      "610  68.1   test  \n",
      "611  66.2   test  \n",
      "612  82.3   test  \n",
      "613  64.7   test  \n",
      "614  68.1   test  \n",
      "\n",
      "[615 rows x 13 columns]\n",
      "\n",
      "Data shape: \n",
      "(615, 14)\n",
      "\n",
      "Data size: \n",
      "8610\n",
      "\n",
      "Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Label shape: \n",
      "(615, 1)\n",
      "\n",
      "Label size: \n",
      "615\n",
      "\n",
      "Label ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Number of Rows with Missing Values: \n",
      "26\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Index(['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT',\n",
      "       'PROT'],\n",
      "      dtype='object')\n",
      "Missing rows before imputation: \n",
      "    Age   ALB   ALP    ALT    AST   BIL    CHE  CHOL  CREA   GGT  PROT\n",
      "4    59  36.0   NaN  100.0   80.0  12.0   9.07   5.3  67.0  34.0  68.0\n",
      "6    32  47.4  52.5   19.1   17.1   4.6  10.19   NaN  63.0  23.0  72.2\n",
      "8    50  42.0   NaN  258.0  106.0  15.0   8.74   4.7  77.0  80.0  84.0\n",
      "42   46  42.9  55.1   15.2   29.8   3.6   8.37   NaN  61.0  29.0  71.9\n",
      "59   49  39.0   NaN  118.0   62.0  10.0   7.28   3.5  72.0  74.0  81.0\n",
      "\n",
      "Missing rows after imputation(simple): \n",
      "     Age   ALB   ALP    ALT    AST   BIL    CHE  CHOL  CREA   GGT  PROT\n",
      "4   59.0  36.0  66.2  100.0   80.0  12.0   9.07  5.30  67.0  34.0  68.0\n",
      "6   32.0  47.4  52.5   19.1   17.1   4.6  10.19  5.25  63.0  23.0  72.2\n",
      "8   50.0  42.0  66.2  258.0  106.0  15.0   8.74  4.70  77.0  80.0  84.0\n",
      "42  46.0  42.9  55.1   15.2   29.8   3.6   8.37  5.25  61.0  29.0  71.9\n",
      "59  49.0  39.0  66.2  118.0   62.0  10.0   7.28  3.50  72.0  74.0  81.0\n",
      "\n",
      "Missing rows after imputation(KNN): \n",
      "     Age   ALB    ALP    ALT    AST   BIL    CHE   CHOL  CREA   GGT  PROT\n",
      "4   59.0  36.0  63.76  100.0   80.0  12.0   9.07  5.300  67.0  34.0  68.0\n",
      "6   32.0  47.4  52.50   19.1   17.1   4.6  10.19  4.910  63.0  23.0  72.2\n",
      "8   50.0  42.0  70.86  258.0  106.0  15.0   8.74  4.700  77.0  80.0  84.0\n",
      "42  46.0  42.9  55.10   15.2   29.8   3.6   8.37  6.054  61.0  29.0  71.9\n",
      "59  49.0  39.0  67.66  118.0   62.0  10.0   7.28  3.500  72.0  74.0  81.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# separate the features and the labels to be used in model development (2)\n",
    "\n",
    "# assume label is 'category' and features are everything else\n",
    "labels = df.copy()\n",
    "labels = labels.drop(['Age','Sex','ALB','ALP','ALT','AST','BIL','CHE','CHOL','CREA','GGT','PROT','split'],axis=1) #Drop all except category\n",
    "\n",
    "label_train = train_df.copy()\n",
    "label_train = label_train.drop(['Age','Sex','ALB','ALP','ALT','AST','BIL','CHE','CHOL','CREA','GGT','PROT','split'],axis=1) #Drop all except category for the train df\n",
    "\n",
    "label_test = test_df.copy()\n",
    "label_test = label_test.drop(['Age','Sex','ALB','ALP','ALT','AST','BIL','CHE','CHOL','CREA','GGT','PROT','split'],axis=1) #Drop all except category for the test df\n",
    "\n",
    "print(f\"Labels: \\n{labels}\\n\")\n",
    "features =df.copy()\n",
    "features = features.drop(['category'],axis=1)\n",
    "print(f\"Features: \\n{features}\\n\")\n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "print(f\"Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Data size: \\n{df.size}\\n\")\n",
    "print(f\"Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Label shape: \\n{labels.shape}\\n\")\n",
    "print(f\"Label size: \\n{labels.size}\\n\")\n",
    "print(f\"Label ndim: \\n{labels.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# check for missing values in the training dataset and print how many rows can be identified with the missing values (1)\n",
    "missing=0\n",
    "missingdf= df.isnull() #returns a dataframe with false where not null and true where null\n",
    "for i in range (len(df.index)) :\n",
    "        if (df.iloc[i].isnull().sum()) > 0 :\n",
    "              missing = missing + 1  \n",
    "\n",
    "sample_incomplete_rows = features[features.isnull().any(axis=1)].head()\n",
    "print(f\"Number of Rows with Missing Values: \\n{missing}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# data imputation\n",
    "# given the task in predicting individuals with hepatitis C infection, select two of the most appropriate imputation strategies to fill the missing values and briefly explain why you have selected the particular strategies in a markdown cell below the current cell (3)\n",
    "imputer_simple = SimpleImputer(strategy='median')\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "imputer_iter = IterativeImputer(max_iter=10)\n",
    "\n",
    "# print the rows before and after being imputed with the two selected strategies (5)\n",
    "numerical_attribute_train = train_df.drop(['split','Sex','category'],axis=1)\n",
    "print(numerical_attribute_train.columns)\n",
    "print(f\"Missing rows before imputation: \\n{numerical_attribute_train.loc[sample_incomplete_rows.index.values]}\\n\")\n",
    "\n",
    "knnImputer = KNNImputer(n_neighbors=5)\n",
    "knnImputer.fit(numerical_attribute_train)\n",
    "knnInput_x = knnImputer.transform(numerical_attribute_train)\n",
    "\n",
    "imputer_simple.fit(numerical_attribute_train)\n",
    "input_x = imputer_simple.transform(numerical_attribute_train)\n",
    "\n",
    "#imputer_iter.fit(numerical_attribute_train)\n",
    "#iter_input = imputer_iter.transform(numerical_attribute_train)\n",
    "\n",
    "# creating the dataframe from the 2D numpy array\n",
    "dfnum_train = pd.DataFrame(input_x, columns=numerical_attribute_train.columns, index=numerical_attribute_train.index)\n",
    "dfnum_train_knn = pd.DataFrame(knnInput_x, columns=numerical_attribute_train.columns, index=numerical_attribute_train.index)\n",
    "#dfnum_train_iter = pd.DataFrame(iter_input, columns=numerical_attribute_train.columns, index=numerical_attribute_train.index)\n",
    "\n",
    "# see what is imputed into the missing value cells\n",
    "print(f\"Missing rows after imputation(simple): \\n{dfnum_train.loc[sample_incomplete_rows.index.values]}\\n\")\n",
    "print(f\"Missing rows after imputation(KNN): \\n{dfnum_train_knn.loc[sample_incomplete_rows.index.values]}\\n\")\n",
    "#print(f\"Missing rows after imputation(Iterative): \\n{dfnum_train_iter.loc[sample_incomplete_rows.index.values]}\\n\")\n",
    "\n",
    "# indicate the encoding strategy that is more appropriate given the categorical feature 'Sex' and briefly explain why you selected one strategy over the other (i.e., either OrdinalEncoder or OneHotEncoder) in the markdown cell mentioned below (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data imputations explanation?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected Simple and KNN imputation strategies. When analyzing the data, the Iterative imputer filled the missing values with ones that were far beyond the average for their column, particularly for the \"ALP\" column. ALP values are generally within 30-60, but the Iterative imputer filled with values like 107.11 or 89.52 (Simple fills 66.2, Knn: 55.1-67.66). CHOL values were more appropriate across the board, but introducing outliers in other columns didn't seem adequate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Categorical data encoding strategy explanation?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OneHotEncoder is the more applicable strategy for \"Sex\" as it is a binary value. OrdinalEncoder applies integer representations to the values in the data, perhaps marking male as \"1\" and female as \"2\". OneHot simply marks one as '1', and all other values as '0'. The ordinal Encoder might be more applicable if we had a trinary+ value, but because Sex is binary in this dataset, OneHot's representation is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Age       Sex       ALB       ALP       ALT        AST       BIL  \\\n",
      "0    1.099266 -0.689939  0.568193 -0.118191 -0.464372   0.414691 -0.341571   \n",
      "1   -0.637835 -1.883968  0.626387 -0.855474  0.847381  10.350398 -3.061399   \n",
      "2    2.739862 -2.200034 -0.697544 -0.121788 -0.229432  -0.149465 -1.768054   \n",
      "3    0.327221  0.995748  1.790283  1.205321 -0.125013  -0.395941  1.042118   \n",
      "4    1.099266 -1.006006 -0.157060  2.568395  1.493468   0.053193  0.395446   \n",
      "..        ...       ...       ...       ...       ...        ...       ...   \n",
      "425 -0.251812  0.837715  0.353599  0.874443 -0.007543   0.058670  0.752066   \n",
      "426 -0.734340 -0.022688 -0.279269 -0.499420 -0.585106  -0.291873 -0.298776   \n",
      "427 -1.313374 -0.970887  0.102634 -0.513806 -0.412163  -0.401418  0.752066   \n",
      "428 -0.734340 -0.479229 -0.908500 -0.172139  2.625752  -0.001579  0.666477   \n",
      "429  0.906255  0.310937 -0.432030  1.144181  0.100138  -0.204237 -0.688682   \n",
      "\n",
      "          CHE      CHOL      CREA       GGT  PROT  split  \n",
      "0   -0.242132  0.833878 -0.387404 -1.502689   0.0    1.0  \n",
      "1    0.012432  0.785540  1.989439 -0.036316   0.0    1.0  \n",
      "2   -2.391781  1.407885 -0.371392 -2.645317   0.0    1.0  \n",
      "3    1.190968  0.169237  0.535936 -0.188667   1.0    0.0  \n",
      "4   -0.025281 -0.404770 -0.092077 -0.759981   1.0    0.0  \n",
      "..        ...       ...       ...       ...   ...    ...  \n",
      "425  0.238711  0.622401 -0.076066  0.801611   0.0    1.0  \n",
      "426 -0.713546 -0.253716 -0.522613  0.211253   1.0    0.0  \n",
      "427  0.248139 -0.465192 -0.209496  0.439779   1.0    0.0  \n",
      "428 -1.062392 -0.286948  1.567798  1.772845   0.0    1.0  \n",
      "429  0.625271  0.894299  2.597882 -0.036316   0.0    1.0  \n",
      "\n",
      "[430 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# select one of the scaling strategies and briefly explain why it is essential to scale your features in the markdown cell mentioned below (3)\n",
    "\n",
    "# create the necessary pipelines and combine the features to be used as the training data for the given algorithm (8)\n",
    "train_df = train_df.drop(['category'],axis=1)\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "num_attribs = list(numerical_attribute_train)\n",
    "cat_attribs = [\"Sex\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "data_prepared = pd.DataFrame(full_pipeline.fit_transform(train_df),columns=train_df.columns, index=train_df.index)\n",
    "#full_pipeline.fit_transform(train_df)\n",
    "# pd.DataFrame(full_pipeline.fit_transform(train_df),columns=train_df.columns, index=train_df.index)\n",
    "print(data_prepared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why scaling?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the Standard scalar. Scaling is essential because we need to calculate the distance between data points, if we were to not scale, we would have no standardized measure to determine how different or similar any given data points are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the following four different models with their default hyperparameter values to be trained using the preprocessed data (0.5 * 4)\n",
    "# Support Vector Machine\n",
    "clf = SVC()\n",
    "labelTrainFlat = label_train.values.ravel()\n",
    "clf.fit(data_prepared, labelTrainFlat)\n",
    "# Decision Trees\n",
    "clg = DecisionTreeClassifier()\n",
    "clg = clg.fit(data_prepared,labelTrainFlat)\n",
    "# Random Forests\n",
    "clh = RandomForestClassifier()\n",
    "clh = clh.fit(data_prepared,labelTrainFlat)\n",
    "# Naive Bayes\n",
    "cli = GaussianNB()\n",
    "cli.fit(data_prepared,labelTrainFlat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters SVC: \n",
      "{'C': 7, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "Best parameters DTC: \n",
      "{'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 3}\n",
      "\n",
      "Best parameters RFC: \n",
      "{'bootstrap': False, 'max_depth': 6, 'n_estimators': 155}\n",
      "\n",
      "Best parameters NB: \n",
      "{}\n",
      "\n",
      "Best estimator SVC: \n",
      "SVC(C=7, gamma=0.1)\n",
      "\n",
      "Best estimator DTC: \n",
      "DecisionTreeClassifier(max_depth=4, min_samples_leaf=4, min_samples_split=3)\n",
      "\n",
      "Best estimator RFC: \n",
      "RandomForestClassifier(bootstrap=False, max_depth=6, n_estimators=155)\n",
      "\n",
      "Best estimator NB: \n",
      "GaussianNB()\n",
      "\n",
      "Best score SVC: \n",
      "0.9004162679425838\n",
      "\n",
      "Best score DTC: \n",
      "0.8620701754385965\n",
      "\n",
      "Best score RFC: \n",
      "0.9197496012759171\n",
      "\n",
      "Best score NB: \n",
      "0.8354992025518342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use sklearn GridSearchCV to train your selected model with hyperparameter tuning\n",
    "# state briefly the advantage of using cross-validation in the markdown cell below (2)\n",
    "\n",
    "# finetune 2 or more of the hyperparameters mentioned below and use at least 2 different values for each hyperparameter except for the Naive Bayes algorithm(use param_grid={}) (8)\n",
    "# parameters for SVC:\n",
    "    # C -> e.g., 10, 100\n",
    "    # gamma ->  e.g., 0.001, 0.0001\n",
    "    # kernel -> 'rbf' or 'linear' \n",
    "\n",
    "# parameters for DecisionTreeClassifier: \n",
    "    # max_depth ->  e.g., 3, 4\n",
    "    # min_samples_split -> 5, 10\n",
    "    # min_samples_leaf -> 10, 20\n",
    "\n",
    "# parameters for RandomForestClassifier: \n",
    "    # n_estimators -> 100, 200\n",
    "    # max_depth -> 3, 5\n",
    "    # bootstrap -> True, False\n",
    "# Using sklearn GridSearchCV for hyperparameter tuning\n",
    "# initialize the model\n",
    "\n",
    "# specify the parameter combinations to be tested\n",
    "parametersSVC = [\n",
    "    {'C': [7,10,20],'kernel': ['rbf', 'linear'], 'gamma': [0.001,0.1,0.01]}\n",
    "]\n",
    "\n",
    "parametersDTC = [\n",
    "    {'max_depth': [1,2,3,4],'min_samples_split': [2,3], 'min_samples_leaf': [4,5]}\n",
    "]\n",
    "\n",
    "parametersRFC = [\n",
    "    {'n_estimators': [145,150,155], 'max_depth': [5,6,7], 'bootstrap': [True, False]}\n",
    "]\n",
    "\n",
    "parametersNB = {}\n",
    "\n",
    "\n",
    "# initialize gridsearch with the required parameters, including the following scoring methods and refit='bal_accuracy' (2)\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "grid_searchSVC = GridSearchCV(clf, parametersSVC, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchDTC = GridSearchCV(clg, parametersDTC, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchRFC = GridSearchCV(clh, parametersRFC, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchNB = GridSearchCV(cli, parametersNB, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "\n",
    "# fit the training data (0.5)\n",
    "grid_searchSVC.fit(data_prepared, labelTrainFlat)\n",
    "grid_searchDTC.fit(data_prepared, labelTrainFlat)\n",
    "grid_searchRFC.fit(data_prepared, labelTrainFlat)\n",
    "grid_searchNB.fit(data_prepared, labelTrainFlat)\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "print(f\"Best parameters SVC: \\n{grid_searchSVC.best_params_}\\n\")\n",
    "print(f\"Best parameters DTC: \\n{grid_searchDTC.best_params_}\\n\")\n",
    "print(f\"Best parameters RFC: \\n{grid_searchRFC.best_params_}\\n\")\n",
    "print(f\"Best parameters NB: \\n{grid_searchNB.best_params_}\\n\")\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "print(f\"Best estimator SVC: \\n{grid_searchSVC.best_estimator_}\\n\")\n",
    "print(f\"Best estimator DTC: \\n{grid_searchDTC.best_estimator_}\\n\")\n",
    "print(f\"Best estimator RFC: \\n{grid_searchRFC.best_estimator_}\\n\")\n",
    "print(f\"Best estimator NB: \\n{grid_searchNB.best_estimator_}\\n\")\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n",
    "print(f\"Best score SVC: \\n{grid_searchSVC.best_score_}\\n\")\n",
    "print(f\"Best score DTC: \\n{grid_searchDTC.best_score_}\\n\")\n",
    "print(f\"Best score RFC: \\n{grid_searchRFC.best_score_}\\n\")\n",
    "print(f\"Best score NB: \\n{grid_searchNB.best_score_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why should you use cross-validation?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using Cross-validation in hyper parameter tuning we would either have to not tune our parameters, or just try random values until we found ones that were applicable. Whereas using cross-validation gives us a good metric to evaluate each parameter. This allows us to tune the parameters easily, to cater to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Accuracy for Support Vector Machine: \n",
      "[0.92093023 0.94883721 0.95116279 0.94883721 0.96511628 0.94883721\n",
      " 0.92790698 0.94883721 0.94883721 0.94883721 0.96046512 0.94883721\n",
      " 0.93255814 0.94651163 0.94883721 0.94651163 0.9627907  0.94651163]\n",
      "\n",
      "Balanced Test Accuracy for Support Vector Machine: \n",
      "[0.70419936 0.83927273 0.90041627 0.83927273 0.89106539 0.83927273\n",
      " 0.72460606 0.83927273 0.89908293 0.83927273 0.88748963 0.83927273\n",
      " 0.74369697 0.83018182 0.89908293 0.83018182 0.89748963 0.83018182]\n",
      "\n",
      "Mean F1 Macro for Support Vector Machine: \n",
      "[0.75916514 0.8724877  0.89022923 0.8724877  0.91340722 0.8724877\n",
      " 0.78199434 0.8724877  0.88619724 0.8724877  0.90378498 0.8724877\n",
      " 0.80202916 0.86466975 0.88619724 0.86466975 0.91085434 0.86466975]\n",
      "\n",
      "Mean Test Accuracy for Decision Trees: \n",
      "[0.95116279 0.95116279 0.95116279 0.95116279 0.94651163 0.94651163\n",
      " 0.94651163 0.94651163 0.94883721 0.95348837 0.95116279 0.95116279\n",
      " 0.95116279 0.95348837 0.94418605 0.94418605]\n",
      "\n",
      "Balanced Test Accuracy for Decision Trees: \n",
      "[0.86162839 0.86162839 0.86162839 0.86162839 0.83388836 0.83388836\n",
      " 0.83388836 0.83388836 0.85075439 0.85338596 0.8443126  0.8443126\n",
      " 0.85208772 0.86207018 0.8403126  0.8403126 ]\n",
      "\n",
      "Mean F1 Macro for Decision Trees: \n",
      "[0.87911114 0.87911114 0.87911114 0.87911114 0.86391806 0.86391806\n",
      " 0.86391806 0.86391806 0.87356829 0.88239065 0.87609961 0.87609961\n",
      " 0.87793189 0.8859781  0.86295014 0.86295014]\n",
      "\n",
      "Mean Test Accuracy for Random Forests: \n",
      "[0.9627907  0.95348837 0.96046512 0.9627907  0.96046512 0.95813953\n",
      " 0.9627907  0.96046512 0.95813953 0.96046512 0.9627907  0.95813953\n",
      " 0.9627907  0.9627907  0.96976744 0.96744186 0.95813953 0.95581395]\n",
      "\n",
      "Balanced Test Accuracy for Random Forests: \n",
      "[0.89155024 0.86205263 0.87379266 0.89156778 0.88245933 0.87247687\n",
      " 0.89247687 0.88155024 0.88114354 0.89023445 0.89065869 0.88890112\n",
      " 0.90799203 0.90023445 0.9197496  0.91065869 0.89667624 0.88756778]\n",
      "\n",
      "Mean F1 Macro for Random Forests: \n",
      "[0.90966333 0.88358734 0.90162705 0.9092042  0.90298298 0.89636197\n",
      " 0.9085693  0.9038151  0.89856913 0.90576004 0.90901156 0.90087872\n",
      " 0.91354967 0.9122531  0.92846893 0.92267735 0.90171617 0.89701868]\n",
      "\n",
      "Mean Test Accuracy for Naive Bayes: \n",
      "[0.93255814]\n",
      "\n",
      "Balanced Test Accuracy for Naive Bayes: \n",
      "[0.8354992]\n",
      "\n",
      "Mean F1 Macro for Naive Bayes: \n",
      "[0.83426267]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the grid search cross-validation results listing the above mentioned evaluation methods (3)\n",
    "cross_val_resultsSVC = grid_searchSVC.cv_results_\n",
    "cross_val_resultsDTC = grid_searchDTC.cv_results_\n",
    "cross_val_resultsRFC = grid_searchRFC.cv_results_\n",
    "cross_val_resultsNB = grid_searchNB.cv_results_\n",
    "#columnArray = cross_val_resultsNB\n",
    "\n",
    "#SVM\n",
    "print(f\"Mean Test Accuracy for Support Vector Machine: \\n{cross_val_resultsSVC['mean_test_accuracy']}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Support Vector Machine: \\n{cross_val_resultsSVC['mean_test_bal_accuracy']}\\n\")\n",
    "print(f\"Mean F1 Macro for Support Vector Machine: \\n{cross_val_resultsSVC['mean_test_F1_macro']}\\n\")\n",
    "\n",
    "#DTC\n",
    "print(f\"Mean Test Accuracy for Decision Trees: \\n{cross_val_resultsDTC['mean_test_accuracy']}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Decision Trees: \\n{cross_val_resultsDTC['mean_test_bal_accuracy']}\\n\")\n",
    "print(f\"Mean F1 Macro for Decision Trees: \\n{cross_val_resultsDTC['mean_test_F1_macro']}\\n\")\n",
    "\n",
    "#RFC\n",
    "print(f\"Mean Test Accuracy for Random Forests: \\n{cross_val_resultsRFC['mean_test_accuracy']}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests: \\n{cross_val_resultsRFC['mean_test_bal_accuracy']}\\n\")\n",
    "print(f\"Mean F1 Macro for Random Forests: \\n{cross_val_resultsRFC['mean_test_F1_macro']}\\n\")\n",
    "\n",
    "#NB\n",
    "print(f\"Mean Test Accuracy for Naive Bayes: \\n{cross_val_resultsNB['mean_test_accuracy']}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Naive Bayes: \\n{cross_val_resultsNB['mean_test_bal_accuracy']}\\n\")\n",
    "print(f\"Mean F1 Macro for Naive Bayes: \\n{cross_val_resultsNB['mean_test_F1_macro']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Prediction: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Dummy Score: \n",
      "0.8767441860465116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use a dummy classifier to identify a simple baseline (i.e., a majority class baseline) so that you can compare your prediction results (3)\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(data_prepared, labelTrainFlat)\n",
    "DummyClassifier(strategy='most_frequent')\n",
    "print(f\"Dummy Prediction: \\n{dummy_clf.predict(data_prepared)}\\n\") #result is all zeroes because we are predicting a minority class\n",
    "print(f\"Dummy Score: \\n{dummy_clf.score(data_prepared, labelTrainFlat)}\\n\") #With all zeroes we get a ~88% accuracy, beating that \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data shape: \n",
      "(185, 13)\n",
      "\n",
      "Test Data size: \n",
      "2405\n",
      "\n",
      "Test Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       163\n",
      "           1       0.94      0.77      0.85        22\n",
      "\n",
      "    accuracy                           0.97       185\n",
      "   macro avg       0.96      0.88      0.92       185\n",
      "weighted avg       0.97      0.97      0.97       185\n",
      "\n",
      "[[162   1]\n",
      " [  5  17]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# prepare the test data to be predicted (2)\n",
    "\n",
    "test_df = test_df.drop(['category'],axis=1) #We chose KNN for the actual impuation\n",
    "num_pipeline_test = Pipeline([\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "cat_attribs_test = [\"Sex\"]\n",
    "numerical_attr_test = test_df.drop(['split','Sex'],axis=1)\n",
    "numerical_attr_test = list(numerical_attr_test)\n",
    "\n",
    "full_pipeline_test = ColumnTransformer([\n",
    "        (\"num\", num_pipeline_test, numerical_attr_test),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs_test),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print the dimensionality of the dataset and the labels (0.5 + 0.5)\n",
    "print(f\"Test Data shape: \\n{test_df.shape}\\n\")\n",
    "print(f\"Test Data size: \\n{test_df.size}\\n\")\n",
    "print(f\"Test Data ndim: \\n{test_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# transform test data for prediction (2)\n",
    "data_prepared_test = full_pipeline_test.fit_transform(test_df)\n",
    "\n",
    "# obtain predictions on test data using the best model from GridSearchCV (i.e., .best_estimator_) (2)\n",
    "predictions_test = grid_searchRFC.best_estimator_.predict(data_prepared_test)\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "print(classification_report(label_test.values.ravel(),predictions_test))\n",
    "print(confusion_matrix(label_test, predictions_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a table format, report the train and test results you have obtained for all 4 models. Your table must include the following columns: (6)\n",
    "- model\n",
    "- best parameters (validation)\n",
    "- best accuracy (validation)\n",
    "- best f1_macro (validation)\n",
    "- best accuracy (test)\n",
    "- best f1_macro (test)\n",
    "\n",
    "|Model | best parameters (validation) | best accuracy (validation) | best f1_macro (validation) | best accuracy (test) | best f1_macro (test)|\n",
    "|------|------------------------------|----------------------------|----------------------------|----------------------|---------------------|\n",
    "|SVC   |'C': 7, 'gamma': 0.1, 'kernel': 'rbf'| 1.0  | 1.0 | 0.96511628  |0.91340722\n",
    "|DTC   |'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 3| 0.97790698 | 0.9454818 | 0.95813953 | 0.89741107\n",
    "|RFC   |'bootstrap': False, 'max_depth': 6, 'n_estimators': 150| 1.0  | 1.0 | 0.96511628 | 0.91454465 \n",
    "|NB    |Default| 0.93139535 | 0.83655275 | 0.93255814 | 0.83426267"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling data imbalance (18/100)\n",
    "Given the dataset that can be considered as having an imbalance, we can use different data augmentation strategies based on the minority class.\n",
    "In this section, you will be given the task of oversampling the dataset using the Imbalanced-Learn Library. \n",
    "\n",
    "Please install the imbalanced-learn library using the following command:\n",
    "* conda install -c conda-forge imbalanced-learnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: \n",
      "(430, 13)\n",
      "\n",
      "Train Data size: \n",
      "5590\n",
      "\n",
      "Train Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Oversampled Train Data shape: \n",
      "(754, 13)\n",
      "\n",
      "Oversampled Train Data size: \n",
      "9802\n",
      "\n",
      "Oversampled Train Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "New Class Distribution: Counter({0: 377, 1: 377})\n",
      "_____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the oversampling object\n",
    "oversample = SMOTE()\n",
    "# oversample the minority class\n",
    "# input_x will be the transformed training data using the combined pipelines, and the labels represent the training labels\n",
    "input_x_over, y_over = oversample.fit_resample(data_prepared, label_train)\n",
    "\n",
    "# print the dimensionality of the original training dataset (0.5)\n",
    "print(f\"Train Data shape: \\n{train_df.shape}\\n\")\n",
    "print(f\"Train Data size: \\n{train_df.size}\\n\")\n",
    "print(f\"Train Data ndim: \\n{train_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the dimensionality of the oversampled training dataset (0.5)\n",
    "print(f\"Oversampled Train Data shape: \\n{input_x_over.shape}\\n\")\n",
    "print(f\"Oversampled Train Data size: \\n{input_x_over.size}\\n\")\n",
    "print(f\"Oversampled Train Data ndim: \\n{input_x_over.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the new class distribution using the Counter (1)\n",
    "print(f\"New Class Distribution: {Counter(y_over['category'])}\")\n",
    "#print(f\"Percent With HepC: \\n{test_df['category'].value_counts()[1]/ test_df.shape[0] }\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters SVC oversampled: \n",
      "{'C': 20, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Best parameters DTC oversampled: \n",
      "{'max_depth': 4, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "\n",
      "Best parameters RFC oversampled: \n",
      "{'bootstrap': True, 'max_depth': 7, 'n_estimators': 145}\n",
      "\n",
      "Best parameters NB oversampled: \n",
      "{}\n",
      "\n",
      "Best estimator SVC oversampled: \n",
      "SVC(C=20, gamma=0.01)\n",
      "\n",
      "Best estimator DTC oversampled: \n",
      "DecisionTreeClassifier(max_depth=4, min_samples_leaf=5)\n",
      "\n",
      "Best estimator RFC oversampled: \n",
      "RandomForestClassifier(max_depth=7, n_estimators=145)\n",
      "\n",
      "Best estimator NB oversampled: \n",
      "GaussianNB()\n",
      "\n",
      "Best score SVC oversampled: \n",
      "0.9787719298245614\n",
      "\n",
      "Best score DTC oversampled: \n",
      "0.9535263157894736\n",
      "\n",
      "Best score RFC oversampled: \n",
      "0.9880350877192983\n",
      "\n",
      "Best score NB oversampled: \n",
      "0.8422280701754385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the same models as before with their default hyperparameters (1)\n",
    "# Support Vector Machine\n",
    "clf2 = SVC()\n",
    "\n",
    "# Decision Trees\n",
    "clg2 = DecisionTreeClassifier()\n",
    "\n",
    "# Random Forests\n",
    "clh2 = RandomForestClassifier()\n",
    "\n",
    "# Naive Bayes\n",
    "cli2 = GaussianNB()\n",
    "\n",
    "\n",
    "\n",
    "# initialize gridsearch with the required parameters as used before (2)\n",
    "grid_searchSVC2 = GridSearchCV(clf2, parametersSVC, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchDTC2 = GridSearchCV(clg2, parametersDTC, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchRFC2 = GridSearchCV(clh2, parametersRFC, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "grid_searchNB2 = GridSearchCV(cli2, parametersNB, cv=5, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "\n",
    "# fit the oversampled training data (0.5)\n",
    "y_over_flat=y_over.values.ravel()\n",
    "# Support Vector Machine\n",
    "clf2.fit(input_x_over, y_over_flat)\n",
    "grid_searchSVC2.fit(input_x_over, y_over_flat)\n",
    "\n",
    "# Decision Trees\n",
    "clg2 = clg2.fit(input_x_over, y_over_flat)\n",
    "grid_searchDTC2.fit(input_x_over, y_over_flat)\n",
    "\n",
    "# Random Forests\n",
    "clh2 = clh2.fit(input_x_over, y_over_flat)\n",
    "grid_searchRFC2.fit(input_x_over, y_over_flat)\n",
    "\n",
    "# Naive Bayes\n",
    "cli2.fit(input_x_over, y_over_flat)\n",
    "grid_searchNB2.fit(input_x_over, y_over_flat)\n",
    "\n",
    "# print the best parameters (0.5)\n",
    "\n",
    "print(f\"Best parameters SVC oversampled: \\n{grid_searchSVC2.best_params_}\\n\")\n",
    "print(f\"Best parameters DTC oversampled: \\n{grid_searchDTC2.best_params_}\\n\")\n",
    "print(f\"Best parameters RFC oversampled: \\n{grid_searchRFC2.best_params_}\\n\")\n",
    "print(f\"Best parameters NB oversampled: \\n{grid_searchNB2.best_params_}\\n\")\n",
    "\n",
    "# print the best estimator (0.5)\n",
    "print(f\"Best estimator SVC oversampled: \\n{grid_searchSVC2.best_estimator_}\\n\")\n",
    "print(f\"Best estimator DTC oversampled: \\n{grid_searchDTC2.best_estimator_}\\n\")\n",
    "print(f\"Best estimator RFC oversampled: \\n{grid_searchRFC2.best_estimator_}\\n\")\n",
    "print(f\"Best estimator NB oversampled: \\n{grid_searchNB2.best_estimator_}\\n\")\n",
    "\n",
    "# print the best score from trained GridSearchCV model (0.5)\n",
    "print(f\"Best score SVC oversampled: \\n{grid_searchSVC2.best_score_}\\n\")\n",
    "print(f\"Best score DTC oversampled: \\n{grid_searchDTC2.best_score_}\\n\")\n",
    "print(f\"Best score RFC oversampled: \\n{grid_searchRFC2.best_score_}\\n\")\n",
    "print(f\"Best score NB oversampled: \\n{grid_searchNB2.best_score_}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       163\n",
      "           1       0.84      0.73      0.78        22\n",
      "\n",
      "    accuracy                           0.95       185\n",
      "   macro avg       0.90      0.85      0.88       185\n",
      "weighted avg       0.95      0.95      0.95       185\n",
      "\n",
      "[[160   3]\n",
      " [  6  16]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# obtain predictions on test data using the best model from GridSearchCV above (i.e., .best_estimator_) (2)\n",
    "predictions_test_over = grid_searchRFC2.best_estimator_.predict(data_prepared_test)\n",
    "\n",
    "# generate the classification report and the confusion matrix for test predictions (3)\n",
    "print(classification_report(label_test.values.ravel(),predictions_test_over))\n",
    "print(confusion_matrix(label_test, predictions_test_over))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a table format, report the train and test results you have obtained for all 4 models. Your table must include the following columns: (6)\n",
    "- model\n",
    "- best parameters (validation)\n",
    "- best accuracy (validation)\n",
    "- best f1_macro (validation)\n",
    "- best accuracy (test)\n",
    "- best f1_macro (test)\n",
    "\n",
    "|Model | best parameters (validation) | best accuracy (validation) | best f1_macro (validation) | best accuracy (test) | best f1_macro (test)|\n",
    "|------|------------------------------|----------------------------|----------------------------|----------------------|---------------------|\n",
    "|SVC   |'C': 7, 'gamma': 0.1, 'kernel': 'rbf'| 1.0  | 1.0 | 0.9787637969094923  | 0.9787559165244731\n",
    "|DTC   |'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 3| 0.974467068630358 | 0.9744613870291474 | 0.9535540838852098 | 0.9535302965190411\n",
    "|RFC   |'bootstrap': False, 'max_depth': 6, 'n_estimators': 150| 1.0  | 1.0 | 0.9893774834437086 | 0.9893754180348695 \n",
    "|NB    |Default| 0.8405093736614939 | 0.8383985750949844 | 0.8421633554083885 | 0.8400034797346384"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
